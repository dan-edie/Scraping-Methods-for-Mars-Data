{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from splinter import Browser\n",
    "from splinter.exceptions import ElementDoesNotExist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Headlings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url to scrape for headlines\n",
    "url = 'https://mars.nasa.gov/news/'\n",
    "\n",
    "# scrape jpl's website\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "# access the website and create a bs4 object\n",
    "browser.visit(url)\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# save top headline and article teaser \n",
    "news_title = soup.find_all('div', class_= 'content_title')[1].get_text()\n",
    "news_p = soup.find_all('div', class_= 'article_teaser_body')[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Featured Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape jpl's website\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# click to get to full size image\n",
    "browser.click_link_by_partial_text('FULL IMAGE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second click\n",
    "# browser.find_link_by_text('more info     ').first.click()\n",
    "browser.click_link_by_partial_text('more info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape and find the image url\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "image = soup.find('img', class_='main_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the image url string\n",
    "featured_image_url = 'https://www.jpl.nasa.gov' + soup.find('img', class_='main_image').get('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Twitter for Current Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping twitter to get current Mars weather\n",
    "url = 'https://twitter.com/marswxreport?lang=en'\n",
    "browser.visit(url)\n",
    "html = browser.html\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mars weather from most recent tweet\n",
    "try:\n",
    "    mars_weather = soup.find_all(text=re.compile(\"InSight\"))[0]\n",
    "except IndexError:\n",
    "    print(\"Parse failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape and Create a Mars Facts Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape to get Mars fact table\n",
    "url = 'https://space-facts.com/mars/'\n",
    "tables = pd.read_html(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe from the table\n",
    "mars_df = tables[0]\n",
    "mars_df.columns = ['info', 'Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_df = mars_df.set_index('info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_df.index.name = None\n",
    "mars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to HTML and drop any \\n\n",
    "html_table = mars_df.to_html()\n",
    "html_table = html_table.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Hemisphere Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "links = soup.find_all('div', class_='item')\n",
    "hemi_titles = []\n",
    "\n",
    "for link in links:\n",
    "    # Getting the hemisphere names and storing them to a list\n",
    "    hemi_name = link.find('h3').get_text()\n",
    "    hemi_titles.append(hemi_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.visit(url)\n",
    "time.sleep(4)\n",
    "\n",
    "hemi_imgs = []\n",
    "\n",
    "for title in range(len(hemi_titles)):\n",
    "    try:\n",
    "            browser.click_link_by_partial_text(hemi_titles[title])\n",
    "    except:\n",
    "            browser.find_link_by_text('2').first.click()\n",
    "            browser.click_link_by_partial_text(hemi_titles[title])\n",
    "    html = browser.html\n",
    "    soup2 = BeautifulSoup(html, 'html.parser')\n",
    "    hemi_soup = soup2.find('div', 'downloads')\n",
    "    hemi_url = hemi_soup.a['href']\n",
    "    hemi_dict={\"title\": hemi_titles[title].replace(' Enhanced', ''), 'img_url': hemi_url}\n",
    "    hemi_imgs.append(hemi_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_browser():\n",
    "    executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "    return Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_data = {}\n",
    "browser = init_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url to scrape for headlines\n",
    "url = 'https://mars.nasa.gov/news/'\n",
    "\n",
    "# access the website and create a bs4 object\n",
    "# browser = init_browser()\n",
    "\n",
    "browser.visit(url)\n",
    "time.sleep(4)\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "mars_data['news_title'] = soup.find_all('div', class_= 'content_title')[1].get_text()\n",
    "mars_data['news_p'] = soup.find_all('div', class_= 'article_teaser_body')[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "\n",
    "browser.visit(url)\n",
    "time.sleep(4)\n",
    "\n",
    "# click to get to full size image\n",
    "browser.click_link_by_partial_text('FULL IMAGE')\n",
    "\n",
    "# second click\n",
    "browser.click_link_by_partial_text('more info')\n",
    "\n",
    "# scrape and find the image url\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# create the image url string\n",
    "mars_data['featured_image_url'] = 'https://www.jpl.nasa.gov' + soup.find('img', class_='main_image').get('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://twitter.com/marswxreport?lang=en'\n",
    "browser.visit(url)\n",
    "time.sleep(4)\n",
    "\n",
    "html = browser.html\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Mars weather from most recent tweet\n",
    "mars_data['mars_weather'] = soup.find_all(text=re.compile(\"InSight\"))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html('https://space-facts.com/mars/')\n",
    "\n",
    "# create a dataframe from the table\n",
    "mars_df = tables[0]\n",
    "mars_df.columns = ['', 'Value']\n",
    "\n",
    "# convert to HTML and drop any \\n\n",
    "html_table = mars_df.to_html()\n",
    "mars_data['html_table'] = html_table.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "\n",
    "browser.visit(url)\n",
    "time.sleep(4)\n",
    "\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "hemi_links = soup.find_all('div', class_='item')\n",
    "hemi_titles = []\n",
    "hemi_imgs = []\n",
    "\n",
    "for link in hemi_links:\n",
    "    # Getting the hemisphere names and storing them to a list\n",
    "    hemi_name = link.find('h3').get_text()\n",
    "    hemi_titles.append(hemi_name)\n",
    "\n",
    "# browser.visit(url)\n",
    "for title in range(len(hemi_titles)):\n",
    "    try:\n",
    "            browser.click_link_by_partial_text(hemi_titles[title])\n",
    "    except:\n",
    "            browser.find_link_by_text('2').first.click()\n",
    "            browser.click_link_by_partial_text(hemi_titles[title])\n",
    "    html = browser.html\n",
    "    soup2 = BeautifulSoup(html, 'html.parser')\n",
    "    hemi_soup = soup2.find('div', 'downloads')\n",
    "    hemi_url = hemi_soup.a['href']\n",
    "    hemi_dict={\"title\": hemi_titles[title].replace(' Enhanced', ''), 'img_url': hemi_url}\n",
    "    hemi_imgs.append(hemi_dict)\n",
    "\n",
    "mars_data['hemi_imgs'] = hemi_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
